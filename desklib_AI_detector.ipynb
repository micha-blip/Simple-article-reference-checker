{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGk/4M2MhiPQiz2/xlkUVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micha-blip/Simple-article-reference-checker/blob/ai-detector/desklib_AI_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1ONRifpW7NCs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel, PreTrainedModel\n",
        "\n",
        "class DesklibAIDetectionModel(PreTrainedModel):\n",
        "    config_class = AutoConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # Initialize the base transformer model.\n",
        "        self.model = AutoModel.from_config(config)\n",
        "        # Define a classifier head.\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        # Initialize weights (handled by PreTrainedModel)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Forward pass through the transformer\n",
        "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs[0]\n",
        "        # Mean pooling\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        pooled_output = sum_embeddings / sum_mask\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(pooled_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.BCEWithLogitsLoss()\n",
        "            loss = loss_fct(logits.view(-1), labels.float())\n",
        "\n",
        "        output = {\"logits\": logits}\n",
        "        if loss is not None:\n",
        "            output[\"loss\"] = loss\n",
        "        return output\n",
        "\n",
        "def predict_single_text(text, model, tokenizer, device, max_len=768, threshold=0.5):\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = encoded['input_ids'].to(device)\n",
        "    attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"]\n",
        "        probability = torch.sigmoid(logits).item()\n",
        "\n",
        "    label = 1 if probability >= threshold else 0\n",
        "    return probability, label\n",
        "\n",
        "def main():\n",
        "    # --- Model and Tokenizer Directory ---\n",
        "    model_directory = \"desklib/ai-text-detector-v1.01\"\n",
        "\n",
        "    # --- Load tokenizer and model ---\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
        "    model = DesklibAIDetectionModel.from_pretrained(model_directory)\n",
        "\n",
        "    # --- Set up device ---\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # --- Example Input text ---\n",
        "    text_ai = \"AI detection refers to the process of identifying whether a given piece of content, such as text, images, or audio, has been generated by artificial intelligence. This is achieved using various machine learning techniques, including perplexity analysis, entropy measurements, linguistic pattern recognition, and neural network classifiers trained on human and AI-generated data. Advanced AI detection tools assess writing style, coherence, and statistical properties to determine the likelihood of AI involvement. These tools are widely used in academia, journalism, and content moderation to ensure originality, prevent misinformation, and maintain ethical standards. As AI-generated content becomes increasingly sophisticated, AI detection methods continue to evolve, integrating deep learning models and ensemble techniques for improved accuracy.\"\n",
        "    text_human = \"It is estimated that a major part of the content in the internet will be generated by AI / LLMs by 2025. This leads to a lot of misinformation and credibility related issues. That is why if is important to have accurate tools to identify if a content is AI generated or human written\"\n",
        "\n",
        "    # --- Run prediction ---\n",
        "    probability, predicted_label = predict_single_text(text_ai, model, tokenizer, device)\n",
        "    print(f\"Probability of being AI generated: {probability:.4f}\")\n",
        "    print(f\"Predicted label: {'AI Generated' if predicted_label == 1 else 'Not AI Generated'}\")\n",
        "\n",
        "    probability, predicted_label = predict_single_text(text_human, model, tokenizer, device)\n",
        "    print(f\"Probability of being AI generated: {probability:.4f}\")\n",
        "    print(f\"Predicted label: {'AI Generated' if predicted_label == 1 else 'Not AI Generated'}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_ai_content(text, model_directory=\"desklib/ai-text-detector-v1.01\", max_len=768, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Detects whether the given text is likely AI-generated.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to analyze.\n",
        "        model_directory (str): The directory or name of the pre-trained model.\n",
        "        max_len (int): The maximum sequence length for tokenization.\n",
        "        threshold (float): The probability threshold for classifying as AI-generated.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the probability of being AI-generated and the predicted label (0 for Not AI Generated, 1 for AI Generated).\n",
        "    \"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
        "    model = DesklibAIDetectionModel.from_pretrained(model_directory)\n",
        "\n",
        "    # Set up device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Run prediction\n",
        "    probability, predicted_label = predict_single_text(text, model, tokenizer, device, max_len=max_len, threshold=threshold)\n",
        "    return probability, predicted_label"
      ],
      "metadata": {
        "id": "0xt1rKj37paK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_check = '''Quantum leap computing at sub-zero temperatures represents a pivotal advancement in the field of quantum information science, enabling unprecedented improvements in coherence times, error reduction, and computational stability. At cryogenic conditions, typically in the millikelvin range achieved through dilution refrigeration, quantum bits (qubits) are effectively isolated from thermal noise, thereby preserving superposition and entanglement states essential for large-scale quantum operations. These ultra-low temperatures suppress phononic and electronic excitations, reducing decoherence rates that otherwise hinder fault-tolerant quantum computation at higher temperatures. Moreover, operating quantum processors at sub-zero temperatures facilitates integration with superconducting circuits, which exhibit zero resistive losses and allow for the implementation of high-fidelity gate operations. Such advancements have positioned cryogenic quantum architectures as a cornerstone in the pursuit of scalable quantum computing, underscoring the necessity of temperature control as a critical parameter in the design of next-generation quantum technologies.'''\n",
        "probability, label = detect_ai_content(text_to_check)\n",
        "print(f\"Text: '{text_to_check}'\")\n",
        "print(f\"Probability of being AI generated: {probability:.4f}\")\n",
        "print(f\"Predicted label: {'AI Generated' if label == 1 else 'Not AI Generated'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSceCCTe-Ru4",
        "outputId": "eae5ddd7-cff6-444e-f7cf-19a7c42ff203"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'Quantum leap computing at sub-zero temperatures represents a pivotal advancement in the field of quantum information science, enabling unprecedented improvements in coherence times, error reduction, and computational stability. At cryogenic conditions, typically in the millikelvin range achieved through dilution refrigeration, quantum bits (qubits) are effectively isolated from thermal noise, thereby preserving superposition and entanglement states essential for large-scale quantum operations. These ultra-low temperatures suppress phononic and electronic excitations, reducing decoherence rates that otherwise hinder fault-tolerant quantum computation at higher temperatures. Moreover, operating quantum processors at sub-zero temperatures facilitates integration with superconducting circuits, which exhibit zero resistive losses and allow for the implementation of high-fidelity gate operations. Such advancements have positioned cryogenic quantum architectures as a cornerstone in the pursuit of scalable quantum computing, underscoring the necessity of temperature control as a critical parameter in the design of next-generation quantum technologies.'\n",
            "Probability of being AI generated: 0.9997\n",
            "Predicted label: AI Generated\n"
          ]
        }
      ]
    }
  ]
}